{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Practicando PySpark con datos del subte de BA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook está destinado a procesar datos vinculados a los subtes de la Ciudad de Buenos Aires, específicamente sobre la cantidad de pasajeros por molinete de todas las estaciones de la red de Subte.\n",
    "Los datasets se encuentran disponible [aquí](https://data.buenosaires.gob.ar/dataset/subte-viajes-molinetes)\n",
    "\n",
    "En el sitio [Buenos Aires Data]() se encuentra una gran variedad de datos abiertos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entorno de trabajo**\n",
    "Se trabaja de manera local con una imagen de docker que incluye todo lo necesario para trabajar con PySpark\n",
    "\n",
    "Dicha docker image está disponible [aquí](https://hub.docker.com/r/jupyter/pyspark-notebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un vistazo rápido a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> molinetes-2014.csv <==\n",
      "﻿FECHA;DESDE;HASTA;LINEA;MOLINETE;ESTACION;PAX_PAGO;PAX_PASES_PAGOS;PAX_FRANQ;PAX_TOTAL\n",
      "2014-04-09;09:15;09:29;B;LINEA_B_FLORIDA_E_TURN02;Florida;7;NA;NA;7\n",
      "\n",
      "==> molinetes-2015.csv <==\n",
      "periodo,fecha,desde,hasta,linea,molinete,estacion,pax_pagos,pax_pases_pagos,pax_franq,total\n",
      "201501,2015-01-01,05:00:00,05:15:00,LINEA_H,LINEA_H_CASEROS_NORTE_TURN01,CASEROS,0.0,0.0,0.0,0.0\n",
      "\n",
      "==> molinetes-2016.csv <==\n",
      "﻿PERIODO;FECHA;DESDE;HASTA;LINEA;MOLINETE;ESTACION;PAX_PAGOS;PAX_PASES_PAGOS;PAX_FRANQ;TOTAL\n",
      "201601;02/01/2016;05:00:00;05:15:00;LINEA_A;LINEA_A_CARABOBO_E_TURN03;CARABOBO;1;0;0;1\n",
      "\n",
      "==> molinetes-2017.csv <==\n",
      "﻿PERIODO;FECHA;DESDE;HASTA;LINEA;MOLINETE;ESTACION;PAX_PAGOS;PAX_PASES_PAGOS;PAX_FRANQ;TOTAL;ID\n",
      "201701;01/01/2017;08:00:00;08:15:00;LINEA_H;LINEA_H_CASEROS_SUR_TURN02;CASEROS;1;0;0;1;1\n",
      "\n",
      "==> molinetes-2018.csv <==\n",
      "fecha,desde,hasta,linea,molinete,estacion,pax_pagos,pax_pases_pagos,pax_franq,total,periodo\n",
      "2018-01-01,08:00:00,08:15:00,LineaA,LineaA_CBarros_S_Turn01,Castro Barros,1.0,0.0,0.0,1.0,201801\n",
      "\n",
      "==> molinetes-2019.csv <==\n",
      "periodo,fecha,desde,hasta,linea,molinete,estacion,pax_pagos,pax_pases_pagos,pax_franq,total\n",
      "201901,2019-01-01,08:00:00,08:15:00,LineaA,LineaA_Lima_N_Turn02,Lima,1.0,0.0,0.0,1.0\n",
      "\n",
      "==> molinetes.csv <==\n",
      "fecha,hora,linea,estacion,total\n",
      "2018-01-03,9,C,RETIRO,3566\n"
     ]
    }
   ],
   "source": [
    "!head -2 molinetes*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los archivos mantienen la misma estructura. Un grupo tiene el caracter ',' como delimitador, mientras que para el otro es el caracter ';'.\n",
    "\n",
    "El orden de las columnas no es el mismo para todos los archivos. Los valores para el campo *LINEA* no mantiene el mismo formato para todos los arhivos\n",
    "\n",
    "El formato de fecha también difiere entre los archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hands on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para interactuar con una base de datos desde PySpark es necesario disponer del conector JDBC para dicha base de datos\n",
    "PG_JDBC_PATH = \"/home/jovyan/work/postgresql-42.2.12.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--driver-class-path {PG_JDBC_PATH} --jars {PG_JDBC_PATH} pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .appName(\"molinetes\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero veamos si hay missing/null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select=[\"Fecha\", \"Desde\", \"Hasta\", \"Linea\", \"Estacion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molinetes-2014.csv\n",
      "Column: Fecha\t Null values: 0\n",
      "Column: Desde\t Null values: 0\n",
      "Column: Hasta\t Null values: 0\n",
      "Column: Linea\t Null values: 0\n",
      "Column: Estacion\t Null values: 0\n",
      "molinetes-2016.csv\n",
      "Column: Fecha\t Null values: 0\n",
      "Column: Desde\t Null values: 0\n",
      "Column: Hasta\t Null values: 0\n",
      "Column: Linea\t Null values: 0\n",
      "Column: Estacion\t Null values: 0\n",
      "molinetes-2017.csv\n",
      "Column: Fecha\t Null values: 0\n",
      "Column: Desde\t Null values: 0\n",
      "Column: Hasta\t Null values: 0\n",
      "Column: Linea\t Null values: 0\n",
      "Column: Estacion\t Null values: 0\n"
     ]
    }
   ],
   "source": [
    "fnames = [f\"molinetes-201{n}.csv\" for n in [4,6,7]]\n",
    "for fname in fnames:\n",
    "    print(fname)\n",
    "    molinetes = spark.read.csv(fname, sep=\";\", header=True, encoding=\"UTF-8\")\n",
    "    molinetes = molinetes.select(*cols_to_select)\n",
    "    for column in molinetes.columns:\n",
    "        nulls_count = molinetes.filter(molinetes[column].isNull()).count()\n",
    "        print(f\"Column: {column}\\t Null values: {nulls_count}\")\n",
    "    \n",
    "    molinetes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molinetes-2015.csv\n",
      "Column: Fecha\t Null values: 0\n",
      "Column: Desde\t Null values: 0\n",
      "Column: Hasta\t Null values: 0\n",
      "Column: Linea\t Null values: 0\n",
      "Column: Estacion\t Null values: 0\n",
      "molinetes-2018.csv\n",
      "Column: Fecha\t Null values: 78207\n",
      "Column: Desde\t Null values: 78207\n",
      "Column: Hasta\t Null values: 78207\n",
      "Column: Linea\t Null values: 78263\n",
      "Column: Estacion\t Null values: 78207\n",
      "molinetes-2019.csv\n",
      "Column: Fecha\t Null values: 0\n",
      "Column: Desde\t Null values: 0\n",
      "Column: Hasta\t Null values: 0\n",
      "Column: Linea\t Null values: 0\n",
      "Column: Estacion\t Null values: 0\n"
     ]
    }
   ],
   "source": [
    "fnames = [f\"molinetes-201{n}.csv\" for n in [5,8,9]]\n",
    "for fname in fnames:\n",
    "    print(fname)\n",
    "    molinetes = spark.read.csv(fname, sep=\",\", header=True, encoding=\"UTF-8\")\n",
    "    molinetes = molinetes.select(*cols_to_select)\n",
    "    for column in molinetes.columns:\n",
    "        nulls_count = molinetes.filter(molinetes[column].isNull()).count()\n",
    "        print(f\"Column: {column}\\t Null values: {nulls_count}\")\n",
    "    \n",
    "\n",
    "    molinetes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo referente al año 2018 requiere un tratamiento diferente por presentar valores nulos.\n",
    "\n",
    "Primero, trabajaremos con los otros datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(filename, separator):\n",
    "    cols_to_select = [\"fecha\", \"desde\", \"linea\", \"estacion\"]\n",
    "    # Omitimos la columna Hasta dado que los registros se efectuaron cada quince minutos    \n",
    "    \n",
    "    molinetes = spark.read.csv(filename, encoding=\"UTF-8\", header=True, sep=separator)\n",
    "    \n",
    "    # La columna referente al total de pasajeros por molinete tiene nombres diferentes entre los archivos.\n",
    "    # De la siguiente forma logramos obtener el nombre de la columna que hace referencia a dichos totales\n",
    "    total_column = next(filter(lambda s: 'total' in s.lower(), molinetes.columns))\n",
    "    cols_to_select.append(total_column)\n",
    "    molinetes = molinetes.select(*cols_to_select)\n",
    "    molinetes = molinetes.withColumnRenamed(total_column, \"total\")\n",
    "    return molinetes\n",
    "\n",
    "def modify_columns(df):\n",
    "    last_char = F.udf(lambda s: s[-1])\n",
    "    df = df.withColumn(\"linea\", last_char(\"linea\"))\n",
    "    df = df.withColumn(\"estacion\", F.upper(F.col(\"estacion\")))\n",
    "    \n",
    "    df = df.withColumnRenamed(\"desde\", \"hora\")\n",
    "    df = df.withColumn(\"hora\", \\\n",
    "                      F.hour(df[\"hora\"]))\n",
    "\n",
    "    df = df.withColumn(\"total\", F.col(\"total\").cast(\"int\"))\n",
    "    \n",
    "    # Transformar el formato del campo \"Fecha\" a dd/MM/yyyy en caso de ser necesario\n",
    "    df = df.withColumn(\"fecha\", \\\n",
    "                       F.when(F.to_date(\"fecha\").isNotNull(), \\\n",
    "                              F.to_date(\"fecha\")) \\\n",
    "                       .otherwise(F.to_date(\"fecha\", \"dd/MM/yyyy\")))\n",
    "\n",
    "    return df\n",
    "\n",
    "def sort_and_group(df):\n",
    "    cols = [\"fecha\", \"hora\", \"linea\", \"estacion\"]\n",
    "    df = df.orderBy(cols) \\\n",
    "            .groupBy(cols).agg(F.sum(\"total\").alias(\"total\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molinetes-2014.csv\tsaved\n",
      "molinetes-2016.csv\tsaved\n",
      "molinetes-2017.csv\tsaved\n"
     ]
    }
   ],
   "source": [
    "fnames = (f\"molinetes-201{n}.csv\" for n in [4,6,7])\n",
    "for fname in fnames:\n",
    "    print(fname, end=\"\\t\")\n",
    "    molinetes = csv_to_df(fname, ';')\n",
    "    molinetes = modify_columns(molinetes)\n",
    "    molinetes = sort_and_group(molinetes)\n",
    "    molinetes.write.saveAsTable(\"molinetes\", mode=\"append\")\n",
    "    print(\"saved\")\n",
    "    molinetes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molinetes-2015.csv\tsaved\n",
      "molinetes-2019.csv\tsaved\n"
     ]
    }
   ],
   "source": [
    "fnames = (f\"molinetes-201{n}.csv\" for n in [5, 9])\n",
    "for fname in fnames:\n",
    "    print(fname, end=\"\\t\")\n",
    "    molinetes = csv_to_df(fname, ',')\n",
    "    molinetes = modify_columns(molinetes)\n",
    "    molinetes = sort_and_group(molinetes)\n",
    "    molinetes.write.saveAsTable(\"molinetes\", mode=\"append\")\n",
    "    print(\"saved\")\n",
    "    molinetes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a tratar los valores nulos del archivo *molinetes-18.csv*.\n",
    "\n",
    "Cargamos el archivo en un dataframe, contamos los valores nulos y luego realizamos las transformaciones correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-------------+-----+\n",
      "|     fecha|   desde| linea|     estacion|total|\n",
      "+----------+--------+------+-------------+-----+\n",
      "|2018-01-01|08:00:00|LineaA|Castro Barros|  1.0|\n",
      "|2018-01-01|08:00:00|LineaA|         Lima|  4.0|\n",
      "|2018-01-01|08:00:00|LineaA|        Pasco|  1.0|\n",
      "+----------+--------+------+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "molinetes18 = csv_to_df(\"molinetes-2018.csv\", ',')\n",
    "molinetes18.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "molinetes18 = molinetes18.dropna(subset=[\"Linea\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: fecha\t Nulls values: 0\n",
      "Column: hora\t Nulls values: 0\n",
      "Column: linea\t Nulls values: 0\n",
      "Column: estacion\t Nulls values: 0\n",
      "Column: total\t Nulls values: 0\n"
     ]
    }
   ],
   "source": [
    "for column in molinetes18.columns:\n",
    "    nulls_count = molinetes18.filter(molinetes18[column].isNull()).count()\n",
    "    print(f\"Column: {column}\\t Nulls values: {nulls_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores nulos fueron eliminados y ya podemos proceder a cargar este dataframe en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "molinetes18 = modify_columns(molinetes18)\n",
    "molinetes18 = sort_and_group(molinetes18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+-------------+-----+\n",
      "|     fecha|hora|linea|     estacion|total|\n",
      "+----------+----+-----+-------------+-----+\n",
      "|2018-01-01|   8|    A|       ACOYTE|   27|\n",
      "|2018-01-01|   8|    A|      ALBERTI|    9|\n",
      "|2018-01-01|   8|    A|     CARABOBO|   38|\n",
      "|2018-01-01|   8|    A|CASTRO BARROS|   33|\n",
      "|2018-01-01|   8|    A|     CONGRESO|   36|\n",
      "+----------+----+-----+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "molinetes18.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "molinetes18.write.saveAsTable(\"molinetes\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Carga a una base de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya finalizado el procesamiento y con los datos cargados en una tabla provisoria, procedemos a cargar toda la data en un base de datos en postgresql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+-----------------+-----+\n",
      "|     fecha|hora|linea|         estacion|total|\n",
      "+----------+----+-----+-----------------+-----+\n",
      "|2019-10-14|  21|    A|   RIO DE JANEIRO|  157|\n",
      "|2019-10-14|  21|    A|   SAENZ PEÃÂ±A |   87|\n",
      "|2019-10-14|  21|    A|      SAN PEDRITO|  189|\n",
      "|2019-10-14|  21|    B|   ANGEL GALLARDO|  204|\n",
      "|2019-10-14|  21|    B|         CALLAO.B|  281|\n",
      "|2019-10-14|  21|    B|    CARLOS GARDEL|  838|\n",
      "|2019-10-14|  21|    B|CARLOS PELLEGRINI|  704|\n",
      "|2019-10-14|  21|    B|          DORREGO|  136|\n",
      "|2019-10-14|  21|    B|       ECHEVERRIA|   51|\n",
      "|2019-10-14|  21|    B| FEDERICO LACROZE|  338|\n",
      "+----------+----+-----+-----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "molinetes = spark.table(\"molinetes\")\n",
    "molinetes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"jdbc:postgresql://127.0.0.1/subte\"\n",
    "tablename = \"public.molinetes\"\n",
    "connection_details = {\n",
    "    \"user\": \"<a_username>\",\n",
    "    \"password\": \"<a_password>\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "molinetes.write.jdbc(connection_string, tablename, mode=\"append\", properties=connection_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, los datos han sido cargados a una db. Asi resultan mas accesibles para crear algún dashboard de visualización"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
